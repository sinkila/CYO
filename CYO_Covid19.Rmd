---
title: "CYO Finnish Covid-19 Confirmed cases forecasting"
author: "Sari Inkila"
date: "04/08/2021"
output: 
  pdf_document:
    toc: TRUE
    toc_depth: 3
    df_print: "kable"
fontsize: 11pt
geometry: margin=0.5in
---
\newpage
```{r setup, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
# set global chunk options: 
library(knitr)
opts_chunk$set(cache=TRUE, autodep = TRUE)
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

```

```{r loading-libraries, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}

# Note: The process of loading the libraries could take a couple of minutes
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")
if(!require(xfun)) install.packages("xfun", repos = "http://cran.us.r-project.org")
if(!require(latexpdf)) install.packages("latexpdf", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(formatR)) install.packages("formatR", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(jsonlite)) install.packages("jsonlite", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(rjstat)) install.packages("rjstat", repos = "http://cran.us.r-project.org")
if(!require(httr)) install.packages("httr", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(timetk)) install.packages("timetk", repos = "http://cran.us.r-project.org")
if(!require(fpp2)) install.packages("fpp2", repos = "http://cran.us.r-project.org")
if(!require(forecast)) install.packages("forecast", repos = "http://cran.us.r-project.org")
if(!require(TTR)) install.packages("TTR", repos = "http://cran.us.r-project.org")
if(!require(vars)) install.packages("vars", repos = "http://cran.us.r-project.org")
if(!require(rsample)) install.packages("rsample", repos = "http://cran.us.r-project.org")
if(!require(tsibble)) install.packages("tsibble", repos = "http://cran.us.r-project.org")
if(!require(ISOweek)) install.packages("ISOweek", repos = "http://cran.us.r-project.org")
if(!require(crop)) install.packages("crop", repos = "http://cran.us.r-project.org")

#Loading libraries
library(ggthemes)
library(httr)
library(rjstat)
library(dplyr)
library(dslabs)
library(tidyverse)
library(caret)
library(data.table)
library(stringr)
library(lubridate)
library(tinytex)
library(xfun)
library(latexpdf)
library(rmarkdown)
library(ggplot2)
library(formatR)
library(jsonlite)
library(tidyr)
library(gridExtra)
library(ISOweek)
library(timetk)
library(fpp2)
library(forecast)
library(TTR)
library(vars)
library(rsample)
library(tsibble)
library(crop)

options(digits = 7)
options(pillar.sigfig = 7)


```

# 1. Executive Summary
  
## 1.1 Introduction to the project
This is a report for the Harvard Data Science course series' Capstone -course Choose Your Own Project -assignment. The Choose Your Own -assignment is the last of the two assignments included in the Capstone course. 

For this project I chose to analyze the Finnish Covid-19 open data available on the Finnish Institute of Health and Welfare (THL) website. I am aware that the choice was a bit risky considering that we have not covered time series or forecasting in this course series. However, by the time I realized this, I had already invested so much time and effort into downloading and analyzing the data that it seemed easier to just familiarize myself with time series forecasting than finding a new data set and project topic.

I am also aware that to a lot of people the whole topic of Covid-19 might be something they don't want to hear about anymore. However, here in Finland, where we have had it pretty easy up until Christmas 2020, the topic of forecasting the Covid-19 epidemic infection rate is now a hot topic. As a result of our fairly low numbers of confirmed Covid-19 cases compared e.g. to our neighbors Sweden and Russia, we did not have lock downs, mandatory testing on our boarders or mask requirements. In hindsight, this turned out to be a liability as the virus variants started spreading. Now we have the highest number of confirmed Covid-19 cases and hospitalized patients since the beginning of the pandemic.

Our liberal constitution and legal system was not prepared for the need to restrict contacts or enforce mandatory testing on the boarders. The need to change the legislation, and enforce a lock down for the first time since the beginning of the pandemic, has been justified with forecast models created by the Finnish Institute of Health and Welfare (THL) data scientists in co-operation with the local universities. The models referenced by the Finnish government in early March 2021 gave forecasts with exponential growth in Covid-19 infections.

This situation and the need to better understand what the data was telling, got me started on this project topic. I did realize that my knowledge on the field of infectious disease and in time series forecasting is very limited, but I took that as a learning opportunity.

## 1.2 Data set 
This project works with confirmed Finnish Covid-19 case data sourced from the Finnish Institute of Health and Welfare (THL) website

Data sources at THL used were:  

* Covid-19 (Corona)data: https://thl.fi/en/web/thlfi-en/statistics/statistical-databases/open-data/confirmed-corona-cases-in-finland-covid-19-  
* Online tool data: https://sampo.thl.fi/pivot/prod/en/epirapo/omaolosymp/fact_epirapo_omaolosymp.json  
* Vaccination status data: https://sampo.thl.fi/pivot/prod/en/vaccreg/cov19cov/fact_cov19cov.json  

The data downloaded includes weekly data points of:  

* Population combined across all health care district,  
* Number of confirmed deaths resulting from Covid-19,  
* Number of confirmed cases of Covid-19,  
* Number of people tested for Covid-19,  
* Number of symptoms self-reported (in a dedicated online tool screening for need to get tested for Covid-19),  
* Number of people referred urgently to be tested (based on the symptoms reported in the screening tool),  
* Number of people that have received at least one dose of Covid-19 vaccine.  

In addition there was some data related to demographics:

* All confirmed Covid-19 cases per sex and age,  
* All Covid-19 deaths per sex and age.



## 1.3 The goal, model used and the results
The key data for this project was the time series on the weekly confirmed cases of Covid-19. The rest of the downloaded data was only used to better understand the situation. This was due to the openly available data being weekly numbers reported, age group or gender specific total sums. There was no actual individual specific data available. 

This meant that there was no way to e.g. predict who might be at higher risk of contracting the disease or dying from it. The only use for the data set was to understand the current and past situation and related statistics, or to try to forecast the future weekly numbers of confirmed Covid-19 cases and/or deaths. 

The population of Finland is about 5.5 million and we had it fairly easy in the beginning of the pandemic, so the total death toll of Covid-19 is currently at 860 (Source: Helsingin Sanomat, April 7th, 2021). The number of weekly deaths has not been rising as much as the infections due to vaccinations and risk group isolation recommendations. So, to predict the weekly deaths did not seem to make sense. So, I chose to predict the number of Covid-19 infections based on the available data. 

The methods used in the model development were Na√Øve (from here on just Naive), Simple Exponential smoothing (ses), ARIMA, and Multivariate VAR. All but one of the methods used in this project are univariate using just the past weekly number of confirmed Covid-19 cases. Multivariate VAR (vector autoregression) method was the only one using the other available time series data in addition the time series on confirmed cases. The last model developed was a combination model using the mean point forecasts of the other developed models. As the available data set including the validation data was just 59 weeks, I did not take seasonality into account in the models. Also, tuning of the models was limited. 

The goal of this project was to build a forecast model on the development data set (90% of the available data) that forecasts 6 weeks of point forecasts on the validation data set (10 % of the available historical data).

Target of the project was to build a point forecast model. The final result achieved with the best model using the Naive method resulted in time series cross-validation RMSE of 620 on the validation set. The six weeks point forecast was about 1750 cases lower than the observed values in the validation data set. In other words, the forecast did not perform well.  

## 1.4 Key steps
As time series forecasting was not part of the Harvard Data Science course series curriculum I had to do extra steps to understand what to do. This included a lot of online research on both the forecast methods in general as well as the R libraries and functions required. I had to also gain an understanding of the performance metrics, prediction intervals and method accuracy evaluation.  

The key steps that were performed:  

  
1. Identifying interesting data sets openly available  

2. Defining the problem: forecasting Covid-19 infections 6 weeks into the future (on the validation data set)  

3. Conducting preliminary analysis including plotting the data to understand potential seasonality, trends and patterns as well as outliers 

4. Choosing and fitting models
  + Starting with gaining a better understanding of time series and forecasting models related to time series
  + Forecasting models chosen were:
    * A baseline using the Naive method
    * Simple exponential smoothing method 
    * Auto ARIMA method
    * Multivariate vector autoregression (VAR) method
    * Combined model using mean point forecast of the other models  
 
5. Using and evaluating forecasting models
+ Validation of the developed model on the validation data set and calculating performance metrics of the model on the validation data set.    


*** 
\newpage
# 2. The Analysis

## 2.1 Data Cleaning

The data was downloaded from the Finnish Institute of Health and Welfare (THL) website using their THL json interface. The interface required use of parameters to extract the different data sets. 
```{r download_data, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
##########################################################
## This project works with confirmed Finnish Covid-19 case data
## sourced from the Finnish Institute of Health and 
## Welfare (THL) website
#########################################################

# 1. Downloading data

## Data source used: 
## https://thl.fi/en/web/thlfi-en/statistics/statistical-databases/open-data/confirmed-corona-cases-in-finland-covid-19-

## Below download follows the THL json interface parameters as per 
## instructions given on the website 
## (Listed 5 steps done for each of the data sets)

# Step 1: address base
url_base <- "https://sampo.thl.fi/pivot/prod/en/epirapo/covid19case/fact_epirapo_covid19case.json"

## Repeat steps 2.-5. for data required / available

##### Population across all health care district
## Population per week combined across all health care district 
## as well as per health care district
## Step 2: Create the request add-on 
## sid : 445344
## All
request_population <- "?row=hcdmunicipality2020-445222.&column=dateweek20200101-509030&filter=measure-445344"

## Step 3: Combine the parts- All
url <- paste0(url_base, request_population)

## Step 4: Get cube (returns a list, where df as first item)
cube_population <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)


## Step 5: Save cube and check 20 first rows
res_population <- cube_population[[1]]
#head(res_population, 20)

## Modifying the data for easier analysis

#Patterns for extracting extra words
pattern1 <- "Year"
pattern2 <- "Week"


# creating a new field for trimmed year & week combination for identifier 
# across data frames and a date format field for time series processing.
population <- res_population %>%
  filter(dateweek20200101 >= "Year 2020 Week 08" 
         & dateweek20200101 <= "Year 2021 Week 12" 
         & dateweek20200101 != "All times") %>%
  mutate(year_week= str_replace(dateweek20200101, pattern1, "")) %>%
  mutate(year_week= str_squish(year_week)) %>%
  mutate(year_week= str_replace(year_week, " ", "-")) %>%
  mutate(year_week= str_replace(year_week, pattern2, "")) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_trim(year_week)) %>%
  mutate(year_week1= str_replace(dateweek20200101, pattern1, "")) %>%
  mutate(year_week1= str_squish(year_week1)) %>%
  mutate(year_week1= str_replace(year_week1, " ", "-")) %>%
  mutate(year_week1= str_replace(year_week1, pattern2, "W")) %>%
  mutate(year_week1= str_replace(year_week1, " ", "")) %>%
  mutate(year_week1= str_trim(year_week1)) %>%
  mutate(year_week= str_squish(year_week)) %>%
  mutate(year_week1 = paste0(year_week1, "-1")) %>%
  mutate(wdate = ISOweek2date(year_week1)) 
  
# Leaving just required columns
population <- population  %>%
  mutate(population = value) %>%
  arrange(dateweek20200101) %>%
  dplyr::select(dateweek20200101, year_week, wdate, population)
```


```{r download_data1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
#########################################################
## All tested per week combined across all health care district
## as well as per health care district
## Request all tested SID: 445356
## Step 2: Create the request add-on 
request_tested <- "?row=hcdmunicipality2020-445222.&column=dateweek20200101-509030&filter=measure-445356"



## Step 3: Combine the parts
url <- paste0(url_base, request_tested)


## Step 4: Get cube (returns a list, where df as first item)
cube_tested <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)


# Save cube and check 20 first rows
res_tested <- cube_tested[[1]]
#head(res_tested, 20)


## Modifying the data for easier analysis

#Patterns for extracting extra words
pattern1 <- "Year"
pattern2 <- "Week"

#creating a new field for trimmed year & week combination
tested <- res_tested %>%
  mutate(year_week= str_replace(dateweek20200101, pattern1, "")) %>%
  mutate(year_week= str_squish(year_week)) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_replace(year_week, pattern2, "-")) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_trim(year_week))


tested <- tested %>%
  filter(dateweek20200101 >= "Year 2020 Week 08" 
         & dateweek20200101 <= "Year 2021 Week 12" 
         & dateweek20200101 != "All times") %>%
  mutate(tested = value) %>%
  arrange(dateweek20200101) %>%
  dplyr::select(year_week, tested)
```


```{r download_data2, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
#########################################################
## All confirmed cases per week combined across all health care districts
## as well as per health care district
## Step 2: Create the request add-on 
## SID = 444833
request_cases <- "?row=hcdmunicipality2020-445222.&column=dateweek20200101-509030&filter=measure-444833"


## Step 3: Combine the parts
url <- paste0(url_base, request_cases)


## Step 4: Get cube (returns a list, where df as first item)
cube_cases <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)


## Step 5: Save cube and check 20 first rows
res_cases <- cube_cases[[1]]
# head(res_cases, 20)



## Modifying the data for easier analysis
#Patterns for extracting extra words
pattern1 <- "Year"
pattern2 <- "Week"

#creating a new field for trimmed year & week combination
confirmed_cases <- res_cases %>%
  mutate(year_week= str_replace(dateweek20200101, pattern1, "")) %>%
  mutate(year_week= str_squish(year_week)) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_replace(year_week, pattern2, "-")) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_trim(year_week))

confirmed_cases <- confirmed_cases%>%
  filter(dateweek20200101 >= "Year 2020 Week 08" & 
           dateweek20200101 <= "Year 2021 Week 12" & 
           dateweek20200101 != "All times") %>%
   mutate(nr_confirmed = value) %>%
  dplyr::select(year_week, nr_confirmed) %>%
  arrange(year_week)
```


```{r download_data3, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
#########################################################
## All confirmed deaths per week
## Step 2: Create the request add-on 
## sid = 492118)
request_deaths <- "?row=hcdmunicipality2020-445222.&column=dateweek20200101-509030&filter=measure-492118"

## Per health care district - There is no health care district summaries 
## of the deaths available. 

## Step 3: Combine the parts
url <- paste0(url_base, request_deaths)

## Step 4: Get cube (returns a list, where df as first item)
cube_deaths <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)


## Step 5: Save cube and check 20 first rows
res_deaths <- cube_deaths[[1]]
# head(res_deaths, 20)


## Modifying the data for easier analysis

#Patterns for extracting extra words
pattern1 <- "Year"
pattern2 <- "Week"

#creating a new field for trimmed year & week combination
confirmed_deaths <- res_deaths %>%
  mutate(year_week= str_replace(dateweek20200101, pattern1, "")) %>%
  mutate(year_week= str_squish(year_week)) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_replace(year_week, pattern2, "-")) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_trim(year_week))

##Only including cases back from the week I started working on this case
confirmed_deaths <- confirmed_deaths %>%
  filter(dateweek20200101 >= "Year 2020 Week 08" 
         & dateweek20200101 <= "Year 2021 Week 12" 
         & dateweek20200101 != "All times") %>%
  mutate(nr_deaths = value) %>%
  dplyr::select(year_week, nr_deaths) %>%
  arrange(year_week)
```


```{r download_data4, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
##########################################################
### Next retrieving supporting data for the weekly data
##########################################################

## All confirmed cases per age group - Note! Limited availability
## sid = 444833
## Step 2: Create the request add-on 
request_cases_age <- "?row=ttr10yage-444309&column=dateweek20200101-509030.&filter=measure-444833"

## Step 3: Combine the parts
url <- paste0(url_base, request_cases_age)

## Step 4: Get cube (returns a list, where df as first item)
cube_cases_age <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)

## Step 5: Save cube and check 20 first rows
res_cases_age <- cube_cases_age[[1]]
#head(res_cases_age, 20)

## Modifying the data for easier analysis
cases_age <- res_cases_age %>%
  filter(ttr10yage != "All ages") %>%
  mutate(nr_confirmed = value) %>%
  dplyr::select(ttr10yage, nr_confirmed)

## All confirmed deaths per age group
## Step 2: Create the request add-on 
## sid = 492118
request_death_age <- "?row=ttr10yage-444309&column=dateweek20200101-509030.&filter=measure-492118"

## Step 3: Combine the parts
url <- paste0(url_base, request_death_age)

## Step 4: Get cube (returns a list, where df as first item)
cube_death_age <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)

## Step 5: Save cube and check 20 first rows
res_death_age <- cube_death_age[[1]]
#head(res_death_age, 20)

## Modifying the data for easier analysis
death_age <- res_death_age %>%
  filter(ttr10yage <= "All ages") %>%
  mutate(nr_deaths = value, 
         nr_deaths = ifelse(nr_deaths =="..", 0, nr_deaths)) %>%
  dplyr::select(ttr10yage, nr_deaths)

## Joining the tables related to age into one
cases_deaths_age <- left_join(cases_age, death_age, by = "ttr10yage")

## Changing the data types to numeric
cases_deaths_age  <- cases_deaths_age %>%
  mutate(nr_confirmed = as.numeric(nr_confirmed), 
         nr_deaths = as.numeric(nr_deaths)) %>%
  dplyr::select(ttr10yage, nr_confirmed, nr_deaths)
```


```{r download_data5, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
##########################################################
## Sex related data - Note! Limited availability
## All confirmed cases per sex
## Step 2: Create the request add-on 
## sid = 444833
request_cases_sex<- "?row=sex-444328&column=dateweek20200101-509030.&filter=measure-444833"

## Step 3: Combine the parts
url <- paste0(url_base, request_cases_sex)

## Step 4: Get cube (returns a list, where df as first item)
cube_cases_sex <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)

## Step 5: Save cube and check 20 first rows
res_cases_sex <- cube_cases_sex[[1]]
#head(res_cases_sex, 20)

## Modifying the data for easier analysis
cases_sex <- res_cases_sex %>%
  filter(sex != "All sexes") %>%
  mutate(nr_cases = value) %>%
  dplyr::select(sex, nr_cases)


## All confirmed deaths per sex
## Step 2: Create the request add-on 
## sid = 492118
request_death_sex<- "?row=sex-444328&column=dateweek20200101-509030.&filter=measure-492118"

## Step 3: Combine the parts
url <- paste0(url_base, request_death_sex)

## Step 4: Get cube (returns a list, where df as first item)
cube_death_sex <- fromJSONstat(url, naming = "label", 
                               use_factors = F, silent = T)

## Step 5: Save cube and check 20 first rows
res_death_sex <- cube_death_sex[[1]]
#head(res_death_sex, 20)

## Modifying the data for easier analysis
death_sex <- res_death_sex %>%
  filter(sex != "All sexes") %>%
  mutate(nr_deaths = value, ) %>%
  dplyr::select(sex, nr_deaths)


## Joining the tables related to sex into one
cases_deaths_sex <- left_join(cases_sex, death_sex, by = "sex")

## Changing the data types to numeric
cases_deaths_sex <- cases_deaths_sex %>%
  mutate(nr_confirmed = as.numeric(nr_cases), 
         nr_deaths = as.numeric(nr_deaths)) %>%
  dplyr::select(sex, nr_confirmed, nr_deaths)


### Case fatality rate per age group and sex
## Case fatality rate = The number of COVID-19 deaths in a population / 
## Divided by the total number of COVID-19 cases * 100 

cases_deaths_age <-  cases_deaths_age %>%
  mutate(case_fatality_rate = nr_deaths/nr_confirmed*100)

cases_deaths_sex <-  cases_deaths_sex %>%
  mutate(case_fatality_rate = nr_deaths/nr_confirmed*100)
```


```{r download_data6, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
##########################################################
## Downloading self-reported Covid-19 symptoms data per
## instructions given on the site
## Self-reported symptoms across and per health care district
#########################################################

## https://sampo.thl.fi/pivot/prod/en/epirapo/omaolosymp/fact_epirapo_omaolosymp.json

# 1. Downloading data

# Step 1: address base
url_base_s <- "https://sampo.thl.fi/pivot/prod/en/epirapo/omaolosymp/fact_epirapo_omaolosymp.json"


##### Self-reported symptoms across health care districts
## Nr of symptoms reported total sid:533175
## Step 2: Create the request add-on 
request_symp <- "?row=hcdmunicipality2020-445222.&column=dateweek20200101-509030&filter=measure-460489"


## Step 3: Combine the parts- All
url <- paste0(url_base_s, request_symp)


## Step 4: Get cube (returns a list, where df as first item)
cube_symp <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)


## Step 5: Save cube and check 20 first rows
res_symp <- cube_symp[[1]]
#head(res_symp, 20)


## Nr symptoms across health care districts
nr_symptoms <- res_symp %>%
  mutate(year_week= str_replace(dateweek20200101, pattern1, "")) %>%
  mutate(year_week= str_squish(year_week)) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_replace(year_week, pattern2, "-")) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_trim(year_week)) %>%
  filter(dateweek20200101 >= "Year 2020 Week 08" & 
           dateweek20200101 <= "Year 2021 Week 12" & 
           dateweek20200101!= "All times") %>%
  mutate(nr_sympt_reported = value) %>%
  dplyr::select(dateweek20200101, year_week, nr_sympt_reported)
```


```{r download_data7, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
##### Number of reports with urgent referral to treatment per week 
## across health care districts
## Nr of reports with urgent referral to treatment sid:452216
## Step 2: Create the request add-on 
request_reftotreat <- "?row=hcdmunicipality2020-445222.&column=dateweek20200101-509030&filter=measure-452216"


## Step 3: Combine the parts- All
url <- paste0(url_base_s, request_reftotreat)


## Step 4: Get cube (returns a list, where df as first item)
cube_reftotreat <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)


## Step 5: Save cube and check 20 first rows
res_reftotreat <- cube_reftotreat[[1]]
#head(res_reftotreat, 20)


## Nr symptoms across health care districts
nr_reftotreat <- res_reftotreat %>%
  mutate(year_week= str_replace(dateweek20200101, pattern1, "")) %>%
  mutate(year_week= str_squish(year_week)) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_replace(year_week, pattern2, "-")) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_trim(year_week)) %>%
  filter(dateweek20200101 >= "Year 2020 Week 08" & 
           dateweek20200101 <= "Year 2021 Week 12" & 
           dateweek20200101!= "All times") %>%
  mutate(nr_reftotreat = value) %>%
  dplyr::select(dateweek20200101, year_week, nr_reftotreat)
```


```{r download_data8, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
### Nr of reported symptoms per age group
## Nr of symptoms reported total sid:533175
## Step 2: Create the request add-on 
request_symp_age <- "?row=ttr10yage-444309&column=dateweek20200101-509030.&filter=measure-533175"


## Step 3: Combine the parts
url <- paste0(url_base_s, request_symp_age)

## Step 4: Get cube (returns a list, where df as first item)
cube_symp_age <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)

## Step 5: Save cube and check 20 first rows
res_symp_age <- cube_symp_age[[1]]
#head(res_symp_age, 20)

## Modifying the data for easier analysis
nr_symp_age_age <- res_symp_age %>%
  filter(ttr10yage != "All ages") %>%
  mutate(nr_symptoms = value) %>%
  dplyr::select(ttr10yage, nr_symptoms)
```


```{r download_data9, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
### Nr of reports with urgent referral to treatment per age group
## Nr of reports with urgent referral to treatment sid:452216
## Step 2: Create the request add-on 
request_reftotreat_age <- "?row=ttr10yage-444309&column=dateweek20200101-509030.&filter=measure-452216"


## Step 3: Combine the parts
url <- paste0(url_base_s, request_reftotreat_age)

## Step 4: Get cube (returns a list, where df as first item)
cube_reftotreat_age <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)

## Step 5: Save cube and check 20 first rows
res_reftotreat_age <- cube_reftotreat_age[[1]]
#head(res_reftotreat_age, 20)

## Modifying the data for easier analysis
reftotreat_age <- res_reftotreat_age %>%
  filter(ttr10yage != "All ages") %>%
  mutate(nr_reftotreat = value) %>%
  dplyr::select(ttr10yage, nr_reftotreat)
```


```{r download_data10, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
##### Compiling age related data to one data frame
## Joining the tables related to age into one
Per_age_group <- left_join(cases_deaths_age, nr_symp_age_age, by = "ttr10yage")
Per_age_group <- left_join(Per_age_group, reftotreat_age, by = "ttr10yage")

## Changing the data types to numeric
Per_age_group <- Per_age_group %>%
  mutate(nr_confirmed = as.numeric(nr_confirmed), 
         nr_deaths = as.numeric(nr_deaths), 
         nr_symptoms = as.numeric(nr_symptoms), 
         nr_reftotreat = as.numeric(nr_reftotreat)) %>%
  dplyr::select(ttr10yage, nr_confirmed, nr_deaths, nr_symptoms, nr_reftotreat, case_fatality_rate)

## Replacing NAs with 0
Per_age_group [is.na(Per_age_group)] <- 0
```


```{r download_data11, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
#########################################################
## Downloading Vaccination progress data 
##  per instructions given on the site
#########################################################

# 1. Downloading data

## Data source used: 
## https://sampo.thl.fi/pivot/prod/en/vaccreg/cov19cov/fact_cov19cov.json

## Below download follows the THL json interface parameters as per 
## instructions given on the website 
## (Listed 5 steps done for each of the data sets)

# Step 1: address base
url_base_v <- "https://sampo.thl.fi/pivot/prod/en/vaccreg/cov19cov/fact_cov19cov.json"

## Repeat steps 2.-5. for data required / available
```


```{r download_data12, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
#########################################################
## Across all health care districts
## All areas sid: 518362
## Nr of first doses given (cov_vac_dose2) sid: 533170
## NOte! Using first dose, as decision was made to only give second dose after 12 weeks
## Step 2: Create the request add-on 
request_vacc <- "?row=area-518362.&column=dateweek20201226-525425&filter=measure-533170"


## Step 3: Combine the parts- All
url <- paste0(url_base_v, request_vacc)


## Step 4: Get cube (returns a list, where df as first item)
cube_vacc <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)


## Step 5: Save cube and check 20 first rows
res_vacc <- cube_vacc[[1]]
#head(res_vacc, 20)


## Per health care district people vaccinated
vacc_peop <- res_vacc %>%
  mutate(year_week= str_replace(dateweek20201226, pattern1, "")) %>%
  mutate(year_week= str_squish(year_week)) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_replace(year_week, pattern2, "-")) %>%
  mutate(year_week= str_replace(year_week, " ", "")) %>%
  mutate(year_week= str_trim(year_week))


vacc_peop <- vacc_peop  %>%
  filter(dateweek20201226 >= "Year 2020 Week 08" & 
           dateweek20201226 <= "Year 2021 Week 12" & 
           dateweek20201226 != "All times") %>%
  mutate(vacc_peop = value, id = paste0(area, dateweek20201226)) %>%
  arrange(dateweek20201226) %>%
  dplyr::select(id, area, year_week, vacc_peop)
```


```{r download_data13, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
#########################################################
## Vaccination per health care district - population
## Weekly population for 2021 / 
## All areas sid: 518362
## Population (measure/POP) sid: 433796
## Step 2: Create the request add-on 
request_pop_hd <- "?row=area-518362&column=dateweek20201226-525425&filter=measure-433796"

## Step 3: Combine the parts- All
url <- paste0(url_base_v, request_pop_hd)

## Step 4: Get cube (returns a list, where df as first item)
cube_pop_hd <- fromJSONstat(url, naming = "label", use_factors = F, silent = T)

## Step 5: Save cube and check 20 first rows
res_pop_hd <- cube_pop_hd[[1]]
#head(res_pop_hd, 20)

# Using population per health care district in the vaccination data for 
# missing 2021 population data
pop_hd <- res_pop_hd %>%
  filter(area == "All areas" & dateweek20201226 == "All times") %>%
  summarise(population_all = sum(as.numeric(value)))
```

Data sources at THL used were:  

* Covid-19 (Corona)data: https://thl.fi/en/web/thlfi-en/statistics/statistical-databases/open-data/confirmed-corona-cases-in-finland-covid-19-  
* Online tool data: https://sampo.thl.fi/pivot/prod/en/epirapo/omaolosymp/fact_epirapo_omaolosymp.json  
* Vaccination data: https://sampo.thl.fi/pivot/prod/en/vaccreg/cov19cov/fact_cov19cov.json  

Data downloaded from the Covid-19 (Corona) data interface were:

* Population combined across all health care district,  
* Number of confirmed deaths resulting from Covid-19,  
* Number of confirmed cases of Covid-19,  
* Number of people tested for Covid-19,  
* All confirmed Covid-19 cases per sex and age,  
* All Covid-19 deaths per sex and age.

Data downloaded from the Online tool data interface were:  

* Number of symptoms self-reported (in a dedicated online tool screening for need to get tested for Covid-19),  
* Number of people referred urgently to be tested (based on the symptoms reported in the screening tool).

Data downloaded from the Vaccination data interface were:  

* Number of people that have received at least one dose of Covid-19 vaccine.  

The data was then joined to following data frames  

* One containing the weekly sums time series data for each week in the periods starting from week 8/2020 to 12/2021,  
* One containing the age group specific data (confirmed cases and deaths),  
* One containing the gender group specific data (confirmed cases and deaths).  

There were some anomalies in the data. E.g. weekly population was available only for 2020, so the 2021 weekly numbers were taken from the THL json interface for the vaccination data as it was the more recent data set. The date format used by the THL interface required some modification to get it into a datetime format required by the time series processing. ISOweek2date() was used to get the week number and corresponding Monday of each week to match and to correctly include the week 53 of 2020.

```{r data_cleaning, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
##########################################################
## Combining downloaded data
## Across all health care districts
##########################################################

## Combining population, tested, confirmed cases and confirmed deaths 
## and self-reported data and vaccination data into 
## a single data frame
## Also replacing NAs in the population of the year 2021 weeks, 
## with the last population data available for year 2020

# Replacing the NAs
population[is.na(population)] <- pop_hd[1,1]
#population[is.na(wdate)]

combined_df <- left_join(population, tested, by = "year_week")
combined_df <- left_join(combined_df, confirmed_cases, by = "year_week")
combined_df <- left_join(combined_df, confirmed_deaths, by = "year_week")
combined_df <- left_join(combined_df, nr_symptoms, by = "year_week")
combined_df <- left_join(combined_df, nr_reftotreat, by = "year_week")
combined_df <- left_join(combined_df, vacc_peop, by = "year_week")


## Replacing NAs with 0
combined_df [is.na(combined_df)] <- 0

## Changing the data types to numeric
combined_df <- combined_df %>%
  mutate(population = as.numeric(population), 
         tested = as.numeric(tested), 
         nr_confirmed = as.numeric(nr_confirmed), 
         nr_deaths = as.numeric(nr_deaths),
         nr_sympt_reported = as.numeric(nr_sympt_reported),
         nr_reftotreat = as.numeric(nr_reftotreat),
         vacc_peop = as.numeric(vacc_peop)) %>%
  dplyr::select(wdate, 
                year_week, 
                population, 
                tested, 
                nr_confirmed, 
                nr_deaths, 
                nr_sympt_reported, 
                nr_reftotreat, 
                vacc_peop)

```

## 2.2. Data visualization and scaling

In order to better understand the data I created metrics commonly used with Covid-19 data i.e. 

* scaling the weekly numbers per 100 000 inhabitants,   

* calculating the case fatality rate (the number of COVID-19 deaths in a population / Divided by the total number of COVID-19 cases * 100 ),   

* mortality rate (the Covid-19 deaths per 1000 inhabitants in a year) for the year 2020.

```{r analyzing_data,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
##########################################################
## 2. Visualizing downloaded data
##########################################################

## Scaling to x tested, confirmed cases and deaths per 100 000 inhabitants
combined_df <- combined_df %>%
  mutate(tested_per_100k = tested/(population/100000), 
         confirmed_per_100k = nr_confirmed/(population/100000),
         deaths_per_100k = nr_deaths/(population/100000),
         vacc_peop_per_100k = vacc_peop/(population/100000))

## Calculating Case fatality rate = The number of COVID-19 deaths in a population / 
## Divided by the total number of COVID-19 cases * 100 

combined_df <- combined_df %>%
  mutate(case_fatality_rate = nr_deaths/nr_confirmed*100)

## Mortality rate i.e. covid-19 deaths per 1000 inhabitants in a year
first_year_summary <- combined_df %>%
  filter(year_week <="2021-06") %>%
  summarise(average_population = mean(population), 
            total_tested = sum(tested), 
            total_confirmed = sum(nr_confirmed), 
            total_deaths = sum(nr_deaths), ) %>%
  mutate(total_confirmed_per_100k = total_confirmed/(average_population/100000), 
         annual_case_fatality_rate = total_deaths/total_confirmed*100, 
         mortality_rate = total_deaths/(average_population/1000))
```

The plots below visualize the tested, confirmed cases and deaths per 100 000 inhabitants up until the week 6, 2021.

The below plot visualizes the weekly number of tested per 100 000 inhabitants.  

```{r analyzing_data2,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
## Per 100 000 population
population_fig_2 <- combined_df %>%
  filter(year_week<= "2021-06") %>%
  ggplot(aes(x = year_week, y = population)) +
  geom_point() +
  ggtitle("Population", subtitle = "Per week") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

# Per 100 000 tested
tested_fig_2 <- combined_df %>%
  filter(year_week<= "2021-06") %>%
  ggplot(aes(x = wdate, y = tested_per_100k)) +
  geom_path() +
  ggtitle("Tested", subtitle = "Per week per 100 k inhabitants") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
tested_fig_2
```

The below plot visualizes the weekly number of confirmed cases per 100 000 inhabitants.  

```{r analyzing_data21,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Per 100 000  nr of confirmed cases of Covid-19
confirmed_fig_2 <- combined_df %>%
  filter(year_week<= "2021-06") %>%
  ggplot(aes(x =  wdate, y = confirmed_per_100k)) +
  geom_path() +
  ggtitle("Confirmed cases", subtitle = "Per week per 100 k inhabitants") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
confirmed_fig_2
```

The below plot visualizes the weekly number of deaths per 100 000 inhabitants.  

```{r analyzing_data22,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Per 100 000 nr of deaths of Covid-19
deaths_fig_2 <- combined_df %>%
  filter(year_week<= "2021-06") %>%
  ggplot(aes(x = wdate, y = deaths_per_100k)) +
  geom_path() +
  ggtitle("Confirmed deaths", subtitle = "Per week per 100 k inhabitants") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
deaths_fig_2

```

The plots below illustrate the number of confirmed cases of Covid-19, the related deaths and case fatality rate per age group.

The age distribution of the number of confirmed cases:  

```{r analyzing_data3,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
## Age distributions confirmed cases
age_distribution_cc <- cases_deaths_age %>%
  ggplot(aes(x= ttr10yage, y= nr_confirmed)) +
  geom_bar(stat = "identity") +
  ggtitle("Confirmed Covid-19 cases", subtitle = "Per age group") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
age_distribution_cc
```

The age distribution of the number of deaths:  

```{r analyzing_data31,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
## Age distributions deaths
age_distribution_d <- cases_deaths_age %>%
  ggplot(aes(x= ttr10yage, y= nr_deaths)) +
  geom_bar(stat = "identity") +
  ggtitle("Confirmed Covid-19 deaths", subtitle = "Per age group") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
age_distribution_d
```

The age distribution of the number of case fatality rate:  

```{r analyzing_data32,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
## Age distributions case fatality rate
age_distribution_f <- cases_deaths_age %>%
  ggplot(aes(x= ttr10yage, y= case_fatality_rate)) +
  geom_bar(stat = "identity") +
  ggtitle("Covid-19 case fatality rate", subtitle = "Per age group") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
age_distribution_f

#grid.arrange(age_distribution_cc, age_distribution_d, age_distribution_f, ncol = 1)
```

The plots below illustrate the number of confirmed cases of Covid-19, the related deaths and case fatality rate per age gender.

The gender distribution of the number of confirmed cases: 

```{r analyzing_data4,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
## Sex distributions confirmed cases
sex_distribution_cc <- cases_deaths_sex %>%
  ggplot(aes(x= sex, y= nr_confirmed)) +
  geom_bar(stat = "identity") +
  ggtitle("Confirmed Covid-19 cases", subtitle = "Per sex") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
sex_distribution_cc
```

The gender distribution of the number of deaths:  

```{r analyzing_data41,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
## Sex distributions deaths
sex_distribution_d <- cases_deaths_sex %>%
  ggplot(aes(x= sex, y= nr_deaths)) +
  geom_bar(stat = "identity") +
  ggtitle("Confirmed Covid-19 deaths", subtitle = "Per sex") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
sex_distribution_d
```

The gender distribution of the case fatality rate:  

```{r analyzing_data42,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
## Sex distributions case fatality rate
sex_distribution_f <- cases_deaths_sex %>%
  ggplot(aes(x= sex, y= case_fatality_rate)) +
  geom_bar(stat = "identity") +
  ggtitle("Covid-19 case fatality rate", subtitle = "Per sex") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
sex_distribution_f

# Faceted plots
#grid.arrange(sex_distribution_cc, sex_distribution_d, sex_distribution_f, ncol = 1)



```

## 2.3 Creating the training and test sets as time series for model development

As the Covid-19 data was time series data and there was a limited amount of 
it available, the data was split into: 

* a development data set: the data up to week 6 / 2021 (90%),  
* a validation set (=final hold-out test set): the last 10% of the available data (weeks 7-12/2021).  

For the forecast development the development data set was split into a separate training and test sets: 

* Train set: the first 41 weeks (80% of the development data set),    
* Test set: the last 11 weeks (20% of the development data set).  

```{r training_and_test_set,  echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
##########################################################
# Create train, test, validation data sets, 
## validation set = final hold-out test set)
##########################################################

## As the Covid-19 data is time series data and there is a limited amount of 
## it available, train and test set will use the data up to week 6 (90%) 
# the last 10% of the available data (weeks 7-12/2021) will be used as 
# validation set

train_set <- combined_df %>%
  filter(year_week <= "2021-06")


# Transforming the train & test set to time series format for analysis and 
# model development
# Adding metric of seven-day average for cases per 100 000 inhabitants
train_n_test_set_data <- train_set %>%
  dplyr::select(wdate, 
                year_week, 
                nr_sympt_reported, 
                nr_reftotreat, 
                tested, 
                nr_confirmed, 
                nr_deaths, 
                vacc_peop,
                tested_per_100k,
                confirmed_per_100k,
                deaths_per_100k) %>%
  mutate(avg_daily_case_rate = confirmed_per_100k/7)



# In reality 2020 was a leap year, but with only total of 57 weeks of data, 
# it doesn't make much difference
season_duration <- 52 

## Convert the data from data frame to multivariate time series format
train_n_test_set_tsdata <- tsibble::as_tsibble(
  train_n_test_set_data, 
  key = year_week,
  index = wdate, 
  regular = T) 

# Creating the time series for train
train_n_test_set_tsdata <- ts(train_n_test_set_tsdata, start = c(2020, 8), 
                              frequency = season_duration) 

# For the forecast development we need separate train and test sets
# Train set is the first 41 weeks (80% of the development data set)
train_set_tsdata <- head(train_n_test_set_tsdata, 41)

# Test set is the last 11 weeks (20% of the development data set)
test_set_tsdata <- tail(train_n_test_set_tsdata, 11)


```


## 2.4 The model development

### 2.4.1 Model performance metrics for forecasting
Determining the model performance using:  

* RMSE (a residual mean squared error)  
* Mean Absolute Error (MAE)  
* Mean absolute percentage error (MAPE)  
* RMSE on time series cross-validation. 

The theory on the forecasting metrics states the following:

>  "A forecast method that minimizes the MAE will lead to forecasts of the median, while minimizing the RMSE will lead to forecasts of the mean. Note that the the forecast errors MAE and RMSE are on the same scale as the data. 
  Percentage errors (MAPE) have the advantage of being unit-free, and so are frequently used to compare forecast performances between data sets. However, measures based on percentage errors have the disadvantage of being infinite or undefined if training data observation in the period of interest are zero,  have extreme values or are close to zero. Another problem with percentage errors that is often overlooked is that they assume the unit of measurement has a meaningful zero.
  A good way to choose the best forecasting model is to find the model with 
the smallest RMSE computed using time series cross-validation." 
(Source: https://otexts.com/fpp2/accuracy.html)

### 2.4.2 Preliminary time series analysis

Time series have few new dedicated plot types that visualize the time series specific qualities e.g. seasonality. Below you can see the ggseasonplot() plots for both confirmed Covid-19 cases and deaths.  

```{r time-series-analyzes, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
##########################################################
## Preliminary analysis
##########################################################

## Plotting the train set time series data using autoplot

# Seasonal plot: confirmed cases of COVID-19 time series
ggseasonplot(train_n_test_set_tsdata[,"nr_confirmed"], year.labels=TRUE, year.labels.left=TRUE) +
  ylab("Nr of confirmed cases") +
  ggtitle("Seasonal plot: Confirmed cases of COVID-19") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Conclusion: The plot indicates that there is seasonality in the data, however, 
# with only 1 year of data it can't easily be used in the forecasts. Also, need 
# to keep in mind that the first half of 2020 there was not enough testing 
# capacity to confirm all of the COVI-19 cases.
```

The seasonal plot above indicates that there is alikely seasonality in the data, however, with only 1 year of data it can't easily be used in the forecasts. Also, we need to keep in mind that the first half of 2020 there was not enough testing capacity to confirm all of the Covid-19 cases. 

```{r time-series-analyzes1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Seasonal plot: Deaths of COVID-19  time series
ggseasonplot(train_n_test_set_tsdata[,"nr_deaths"], year.labels=TRUE, year.labels.left=TRUE) +
  ylab("Nr of deaths") +
  ggtitle("Seasonal plot: Nr of deaths")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This is suggested also by the seasonal plot of the Covid-19 related deaths that has a spike starting with the week 11/2020 until week 18, but the size of the corresponding peak in the confirmed cases leading up to those weeks in 2020 was not proportional. At that time there was not enough testing capacity.

If you look at the plot below, where I have faceted the two data sets and marked the peaks in confirmed cases in red and peaks in deaths in blue, the size of the initial peak in confirmed cases is lower than you would expect when contrasting with the corresponding peaks in the deaths in the latter part of the data.

```{r time-series-analyzes2, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}

# Looking at the cases vs. deaths  peaks aligned with the same weeks
autoplot(train_n_test_set_tsdata[,6:7], facets=TRUE) +
  ylab("COVID-19 data")+
  geom_vline(xintercept = 2020.25, colour="red") +
  geom_vline(xintercept = 2020.29, colour="blue") +
  geom_vline(xintercept = 2020.77, colour="red") +
  geom_vline(xintercept = 2020.905, colour="red") +
  geom_vline(xintercept = 2020.922, colour="blue") +
  geom_vline(xintercept = 2020.945, colour="red") +
  geom_vline(xintercept = 2020.96, colour="blue") +
  geom_vline(xintercept = 2021.02, colour="red") +
  geom_vline(xintercept = 2021.06, colour="blue") +
  geom_vline(xintercept = 2021.08, colour="red") +
  ggtitle("Faceted data points aligned by week") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Similar lack of accurate data is present in the other data points that are faceted in the plot below. The initial peak in the number of Covid-19 deaths is greater than any of the latter peaks, however, none of the other data points have similar initial peaks in proportion to the deaths. 

The tools for pre-screening people for testing in order of highest priority, where developed early, but the usage was a little slow to pick up. This would suggest that the related time series data that I expected to be leading indicators (Number of people tested, Number of symptoms self-reported and Number of people referred urgently to be tested) have only limited ability to support any prediction or forecasting efforts. 

```{r time-series-analyzes3, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Looking at the different COVID-19 related data sets aligned with the same weeks
autoplot(train_n_test_set_tsdata[,3:8], facets=TRUE) +
  ylab("COVID-19 data")+
  ggtitle("Faceted data points aligned by week")
```

When we combine the different time series into a one plot we can see that the usage of the tool (used for screening for testing) never picked up to extend that it would have volumes higher than the number of people that were actually tested. This is another reason that the data sets are not as useful as I hoped for.

```{r time-series-analyzes4, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Looking at all the data points together
autoplot(train_n_test_set_tsdata[,3:8]) +
  ylab("Weekly number") + xlab("Week")+
  ggtitle("Combined data points")
```

To further understand the relationship of the time series data, I ran the GGally::ggpairs  generalized pairs plot that provides a matrix with both plots and numeric correlations. You can see the results below:

```{r time-series-analyzes5, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# To see the relationships between the different COVID-19 data points in the 
# time series, I plotted each of them against the others.
# These plots are arranged in a scatter plot matrix
GGally::ggpairs(as.data.frame(train_n_test_set_tsdata[,3:8]))
```

The conclusions from the matrix:  

* As the first three time series data are used to determine who should be tested it is to be expected that they have a fairly high correlation.   

* Number of people vaccinated is so low and has been on-going for such a short time, that it is not going to have much impact on anything yet. For this reason it is not used in the model development.

When we look only at the weekly confirmed cases and deaths per 100 000 inhabitants, we can see that the number of deaths remains so low that forecasting it in this limited data set would be difficult. This is the reason why this project focuses on the point forecast of the confirmed Covid-19 cases.

```{r time-series-analyzes6, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Looking at the confirmed cases and deaths per 100 000 inhabitants
autoplot(train_n_test_set_tsdata[,10:11]) +
  ylab("Number reported") + xlab("Week")+
  ggtitle("The confirmed cases and deaths per 100k")
```

As said the epidemic stayed at a fairly low level for most of 2020 and only recently reached tier labeled widespread (over 7 cases with the metric of seven-day average for cases per 100 000 inhabitants). As you can see from the plot below, the first time the threshold was surpassed was when the first cases of the mutated variants started spreading in Finland around November and December 2020. But after that the case numbers appeared to go down during the holiday season (around Christmas and New Year). The cut-off point between the development and data set is right at the point, when the case numbers have started to decline again, but are under the highest threshold.

```{r time-series-analyzes7, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Adjusted case rate seven-day average for cases per 100 000 per 100 000 inhabitants
autoplot(train_n_test_set_tsdata[,12]) +
  geom_hline(yintercept = 1, colour="yellow") +
  geom_hline(yintercept = 4, colour="orange") +
  geom_hline(yintercept = 7, colour="red") +
  ylab("Adjusted daily rate") + xlab("Week")+
  ggtitle("The daily confirmed Covid-19 case rate per 100k")
```

Gglagplot() was used to gain an understanding of the possible autocorrelation of the confirmed Covid-19 cases time series. Using the gglagplot we can see that the highest autocorrelation is with lag 1.

```{r time-series-analyzes8, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Lagged plots: the horizontal axis shows lagged values of the time series
confirmed_covid <- window(train_n_test_set_tsdata[,"nr_confirmed"], start=c(2020,8))
gglagplot(confirmed_covid)
```

We can confirm this with the ggAcf(), which shows results of the autororrelation. There was a significant autocorrelation up to 11 lags, but the first lag is the highest. 

```{r time-series-analyzes9, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Plotting autocorrelation. 
# Autocorrelation (ACF) measures the linear relationship between lagged values of 
# a time series
# Autocorrelation: confirmed cases of COVID-19 time series
ggAcf(train_n_test_set_tsdata[,"nr_confirmed"])

# Conclusion: For the first 11 lags the correlations are significantly 
# different from zero


## Theory states: When data have a trend, the autocorrelations for small lags 
## tend to be large and positive because observations nearby in time are also 
## nearby in size. So the ACF of trended time series tend to have positive 
## values that slowly decrease as the lags increase 
## Conclusion: Confirmed cases data appears to have a trend
## (source: https://otexts.com/fpp2/autocorrelation.html)

## These graphs show the challenges that the forecasting models will phase: 
# - Data appears to have seasonality, but there is only one year's data, 
# so it will be difficult to take into account
# - Data is incomplete as there was not enough testing capacity at the 
# beginning of the pandemic and only the severe cases got tested.

# To a novice in a time series forecasting this will be true challenge to build 
# a model that is able to forecast with any accuracy the change in the trend.

```

The theory states:

>  "When data has a trend, the autocorrelations for small lags 
tend to be large and positive because observations nearby in time are also nearby in size. Which means that the ACF of trended time series tend to have positive values that slowly decrease as the lags increase."
(source: https://otexts.com/fpp2/autocorrelation.html) 

According to this theory, I can conclude that the confirmed cases data appears to have a trend.

These plots together show that there are challenges to be expected in developing the forecasting models:  

* Data appears to have seasonality, but there is only data for one year. So it will be difficult to take it into account.  
* Data is incomplete as there was not enough testing capacity at the beginning of the pandemic and only the severe cases got tested.

To a novice creating time series forecast model with this data set will be a real challeng. 

### 2.4.3 Forecasts with Naive Method

To get started I created a baseline forecast on naive(), which forecasts the last observed value. The definition of naive method is that we simply set all forecasts to be the value of the last observation. The forecast was created for the 11 weeks following the time series in the training set and resulting point forecast was tested against the test set actual data for the same 11 weeks.

To determine the best fit forecast method I ran the time series cross-validation on the development test set (including training and test sets) and checked the residuals of the point forecast based on the training set. After that I calculated the accuracy of the training set forecast using the 11 weeks test set. All model performance metrics were saved to a data frame to enable comparison between the models.

```{r naive, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
##########################################################
## Model development
##########################################################


#############
## 1. Model: Naive

## To get started I'll create the baseline forecast on naive(), which forecasts
## the last observed value. 
## Definition of naive method: For naive forecasts, we simply set all forecasts 
## to be the value of the last observation

### On the number of confirmed COVID-109 cases, h= 11 i.e. 11 weeks
forecast_naive_case <- forecast(naive(train_set_tsdata[,"nr_confirmed"], h = 11))

# Fit the model function to be used with time series cross-validation to 
# determine model with lowest cross-validation RMSE
forecast_naive <- function(x, h){forecast(naive(x, h = h))}

# Using tsCV() function for time series cross-validation
errors_naive <- tsCV(train_n_test_set_tsdata[,"nr_confirmed"], 
                     forecast_naive, h=11)

# Calculate the RMSE of the tsCV results
RMSE_naive <- sqrt(mean(errors_naive^2, na.rm=TRUE))

```

According to theory:  

> "Residuals are useful in checking whether a model has adequately captured the information in the data. A good forecasting method will yield residuals with the following properties:  
* The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.  
* The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts."

> "In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.  
* The residuals have constant variance.  
* The residuals are normally distributed."
(Source: https://otexts.com/fpp2/residuals.html)

The first check of the residuals was the mean of residuals:
```{r naive1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
## Residuals are useful in checking whether a model has adequately captured the 
## information in the data. A good forecasting method will yield residuals with 
## the following properties:
## - The residuals are uncorrelated. If there are correlations between 
## residuals, then there is information left in the residuals which should be 
## used in computing forecasts.
## - The residuals have zero mean. If the residuals have a mean other than zero, 
## then the forecasts are biased.
## In addition to these essential properties, it is useful (but not necessary) 
## for the residuals to also have the following two properties.
## - The residuals have constant variance.
## - The residuals are normally distributed.
## Source: https://otexts.com/fpp2/residuals.html

# Calculating residuals
res_naive <- residuals(forecast_naive_case)

# Mean of residuals
mean(res_naive, na.rm=TRUE)
# Conclusion: Mean of the residuals is not zero.
```

Which is not zero and would indicate that there is a bias.

Next I checked the residuals with checkresiduals() which produces:  

* a time plot,   

* ACF plot and histogram of the residuals (with an overlaid normal distribution for comparison)  

* a Ljung-Box test with the correct degrees of freedom.

For the use of the Ljung-Box test the theory states:  

> "Ideally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model." (Source: https://www.statology.org/ljung-box-test/)

```{r naive2, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Checking residuals with checkresiduals() which produces:
# - a time plot, 
# - ACF plot and histogram of the residuals (with an overlaid normal 
# distribution for comparison)
# - a Ljung-Box test with the correct degrees of freedom
checkresiduals(forecast_naive_case, plot = TRUE)

# Conclusion:
# - The variations in the residuals do not stay the same over time
# - ACF of residuals suggests that naive model hasn't captured all 
# the elements affecting the forecast 
# - Residuals do not appear to be normally distributed.
# - from Ljung-Box test is that as the p-value is small (under 0,05).
```

Conclusion:  

* The variations in the residuals do not stay the same over time,  
* ACF of residuals suggests that naive model hasn't captured all the elements affecting the forecast,   
* Residuals do not appear to be normally distributed,  
* from Ljung-Box test with a small p-value (under 0,05). Thus, the null hypothesis of the test holds and we conclude that the data values are not independent i.e. the residuals are not just white noise.

I used autoplot() to visualize the point forecast results and compared them to the test set actuals. The red line is the actual test set data, black line is the mean point forecast and the fan shows the 95% and 80% prediction intervals.

```{r naive3, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# plot results forecast results
plot_naive <- autoplot(forecast_naive_case,  ylab="Test set") + 
  autolayer(test_set_tsdata[,"nr_confirmed"])
plot_naive

# Conclusion: There are elements left to use in forecasting that the naive 
# model does not capture. 
# The whole forecasting window is not within the 95% prediction 
# interval. However, the last values of the test
# set are within the 95% prediction interval and closing on the forecast, which
# is the last observed value in the training set
```

The conclusion from visual inspection of the plot was that there were elements left to use in forecasting that the naive model did not capture. The observed values were not in the point forecast's 95% prediction interval. However, the last observed values of the test set are within the 95% prediction interval and closing on the point forecast.

I checked the forecast errors, i.e. the difference between an observed value and its forecast. The forecast errors were calculated against the test set.The last column in the table (RMSE_CV) was produced by the cross-validation using the tsCV().

```{r naive4, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Next I'll be checking the forecast errors, i.e. the difference between 
# an observed value and its forecast. The forecast errors will be calculated on 
# the test set.

# Accuracy of the forecast
accuracy_naive_case <- accuracy(forecast_naive_case, 
                                test_set_tsdata[,"nr_confirmed"])


# Creating a results table (following the convention used in the course 8) with 
# RMSE, MAE and MAPE:
accuracy_results <- tibble(method = "Naive", 
                       forecasting = "Test set",
                       RMSE = accuracy_naive_case[2,"RMSE"], 
                       MAE = accuracy_naive_case[2,"MAE"],
                       MAPE = accuracy_naive_case[2,"MAPE"], 
                       RMSE_CV = RMSE_naive)

## Conclusion: As you would expect the Naive method does not capture all the 
## elements affecting the forecast. Accuracy of the method using MAE 
## are off by about 820 confirmed cases.


# Display accuracy results: 
accuracy_results %>% 
  arrange(RMSE)
```

The table above with the results of the accuracy metrics show that the Naive method did not capture all the elements affecting the forecast. Accuracy of the method using mean average error (MAE) are off by 
```{r mae_naive, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
accuracy_naive_case[2,"MAE"]
```
confirmed cases.

### 2.4.4 Forecasts with Simple Exponential Smoothing Method

According to theory:  

> Exponential smoothing methods are an extension of the naive method, where the forecasts are produced not with the mean of the past values, but with the weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the recent values get higher weights and vice versa. 
(Source: https://www.pluralsight.com/guides/time-series-forecasting-using-r)

```{r ses, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
#############
## 2. Model:  Simple Exponential Smoothing

# Simple Exponential Smoothing
# Exponential Smoothing methods are an extension of the naive method, wherein 
# the forecasts are produced using weighted averages of past observations, with 
# the weights decaying exponentially as the observations get older. In simple 
# words, higher weights are given to the more recent observations and vice 
# versa. The value of the smoothing parameter for the level is decided by the 
# parameter 'alpha'.
# Source: https://www.pluralsight.com/guides/time-series-forecasting-using-r

#Forecast for 11 weeks (h - periods)
forecast_ses_conf_case <- forecast(ses(train_set_tsdata[,"nr_confirmed"], h = 11))

# Fit the model function to be used with time series cross-validation to 
# determine model with lowest cross-validation RMSE
forecast_ses <- function(x, h){forecast(ses(x, h = h))}

# Using tsCV() function for time series cross-validation
errors_ses <- tsCV(train_n_test_set_tsdata[,"nr_confirmed"], 
                   forecast_ses, h=11)

# Calculate the RMSE of the tsCV results
RMSE_ses <- sqrt(mean(errors_ses^2, na.rm=TRUE))


```

Again I first checked the mean of residuals:
```{r ses1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Plotting residuals
res_ses <- residuals(forecast_ses_conf_case)

# Mean of residuals
mean(res_ses)
# Conclusion: Mean of the residuals is not zero.
```

The mean of the residuals is not zero indicating that the ses() model has a bias.

Next the I checked the checkresiduals(). 

```{r ses2, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Checking residuals with checkresiduals()
checkresiduals(forecast_ses_conf_case)

# Conclusion:
# - The variations in the residuals do not stay the same over time
# - ACF of residuals suggests that naive model has not captured all 
# the elements affecting the forecast 
# - Residuals do not appear to be normally distributed, 
# - from Ljung-Box test with a small p-value (under 0,05). Thus, the null
# hypothesis of the test holds and we conclude that the data values are not 
# independent.
```

Conclusion:  
* The variations in the residuals do not stay the same over time,  
* ACF of residuals suggests that naive model has not captured all the elements affecting the forecast  
* Residuals do not appear to be normally distributed,  
* from Ljung-Box test with a small p-value (under 0,05). Thus, the null hypothesis of the test holds and we conclude that the data values are not independent, i.e. the residuals are not just white noise.

Below plot shows the visualization of the forecast results compared to the test set:

```{r ses3, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# plot results and the test data
plot_ses <-  autoplot(forecast_ses_conf_case,  ylab="Test set") + 
  autolayer(test_set_tsdata[,"nr_confirmed"])
plot_ses
```

The visualization of the simple exponential smoothing looks very similar to the naive model.

The difference between an observed value and its point forecast was checked using accuracy(), where the forecast errors were calculated against the test set.

```{r ses4, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# accuracy of the forecast
accuracy_ses_cons_case <- accuracy(forecast_ses_conf_case, 
                                   test_set_tsdata[,"nr_confirmed"])




#Adding the results as a new row to the RMSE tibble
accuracy_results <- bind_rows(accuracy_results,
                              tibble(method = "Simple exponential smoothing", 
                                     forecasting = "Test set",  
                                     RMSE = accuracy_ses_cons_case[2,"RMSE"], 
                                     MAE = accuracy_ses_cons_case[2,"MAE"],
                                     MAPE = accuracy_ses_cons_case[2,"MAPE"], 
                                     RMSE_CV = RMSE_ses))


# Display accuracy results: 
accuracy_results %>% 
  arrange(RMSE)


```

Here we can see that the results of the accuracy metrics are almost identical to the Naive method, but the cross-validation RMSE was actually tiny bit better with the Naive model.

### 2.4.5 Forecasts with Auto ARIMA Method

Researching online for models used in times series, and in COVID-19 time series in particular, one of the basic models used was ARIMA. 
(Reference: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0244173)

The theoretical definition of the ARIMA:

> "When differencing is combined with autoregression and a moving average model, we obtain a non-seasonal ARIMA model. ARIMA = AutoRegressive Integrated Moving Average (in this context, ‚Äúintegration‚Äù is the reverse of differencing)."
(Source: https://otexts.com/fpp2/non-seasonal-arima.html)

```{r auto_arima, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
#############
## 4. Model: Auto ARIMA

## Researching online for models used in times series, and in COVID-19 
## time series in particular, model used was ARIMA. As I have little experience with time series 
## analysis, I'll see how those models work on my data. 
## Reference: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0244173

## Definition of ARIMA: When differencing is combined with autoregression and 
## a moving average model, we obtain a non-seasonal ARIMA model. 
## ARIMA = AutoRegressive Integrated Moving Average (in this context, 
## ‚Äúintegration‚Äù is the reverse of differencing). 
## Source: https://otexts.com/fpp2/non-seasonal-arima.html

# Confirmed cases per auto ARIMA forecast without cross-validation
fit_arima_conf_case <- auto.arima(train_set_tsdata[,"nr_confirmed"])

# forecast with auto ARIMA for the next 11 weeks (h - periods)
forecast_arima_conf_case <- forecast(fit_arima_conf_case, h=11)

# Fit the model function to be used with time series cross-validation to 
# determine model with lowest cross-validation RMSE
forecast_arima <- function(x, h){forecast(auto.arima(x), h = h)}

# Using tsCV() function for time series cross-validation
errors_arima <- tsCV(train_n_test_set_tsdata[,"nr_confirmed"], 
                   forecast_arima, h=11)

# Calculate the RMSE of the tsCV results
RMSE_arima <- sqrt(mean(errors_arima^2, na.rm=TRUE))


```

The resulting mean of residuals is:
```{r auto_arima1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Calculating residuals
res_arima <- residuals(forecast_arima_conf_case)

# Mean of residuals
mean(res_arima, na.rm=TRUE)
# Conclusion: Mean of the residuals is not zero.
```

Mean of the residuals is not zero, which is suggesting that there is a bias.

The results of the checkresiduals() are: 

```{r auto_arima2, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Checking residuals with checkresiduals()
checkresiduals(forecast_arima_conf_case)
# Conclusion:
# - The variations in the residuals do not stay the same over time
# - ACF of residuals suggests that Arima model has captured all 
# the elements affecting the forecast 
# - Residuals do not appear to be normally distributed.
# - from Ljung-Box test is that as the p-value is over 0.05, I can conclude 
# that the residuals are not distinguishable from a white noise series.
# - The prediction intervals for ARIMA models assume that the residuals are 
# uncorrelated and normally distributed. As neither of these assumptions holds
# the the prediction intervals may be incorrect.
```

The conclusions from checking the residuals:  
* The variations in the residuals do not stay the same over time,  
* ACF of residuals suggests that Arima model has captured all the elements affecting the forecast,   
* Residuals do not appear to be normally distributed,  
* from Ljung-Box test is that as the p-value is over 0.05. Thus, we reject the null hypothesis of the test and conclude that the data values are independent, i.e. the residuals are just white noise.

Below plot shows the visualization of the point forecast results compared to the observed values in the test set:

```{r auto_arima3, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# plot results and the test data
plot_arima <- autoplot(forecast_arima_conf_case,  ylab="Test set") + autolayer(test_set_tsdata[,"nr_confirmed"])
plot_arima

# Conclusion: the plot confirms that the test data does not fit in the 
# 95% prediction interval
```

Conclusion: the plot confirms that the test data does not fit in the 95% prediction interval.

Again the difference between an observed value and its forecast was checked using accuracy(), where the forecast errors were calculated against the test set.

```{r auto_arima4, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# accuracy of the forecast
accuracy_arima_conf_case <- accuracy(forecast_arima_conf_case, 
                                     test_set_tsdata[,"nr_confirmed"])

#Adding the results as a new row to the RMSE tibble
accuracy_results <- bind_rows(accuracy_results,
                              tibble(method = "Auto ARIMA", 
                                     forecasting = "Test set",
                                     MAE = accuracy_arima_conf_case[2,"MAE"],
                                     MAPE = accuracy_arima_conf_case[2,"MAPE"],
                                     RMSE = accuracy_arima_conf_case[2,"RMSE"], 
                                     RMSE_CV = RMSE_arima))


# Display accuracy results: 
accuracy_results %>% 
  arrange(RMSE)


```

From the results of the accuracy metrics we can see that the model performs worse than the first two models.

### 2.4.6 Forecasts with Multivariate VAR Method

The VAR method name comes from vector autoregression. So far all the other models I have used have been univariate point forecasts of the confirmed COVID-19 cases based on the observations of the its own time series data. Next I used the VAR method to create a multivariate time series model that uses all of the other related time series with the confirmed cases time series:

* nr of tested,  
* nr of deaths,  
* nr of symptoms reported,  
* nr of urgently referred to treatment.  

In other words I selected to use all the other time series data except vaccinations, which did not have time series data available for the whole period.

I used the VARselect() to select the number of lags p using for the different VAR(p). AIC: 
```{r var, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
#############
## 6. Model: Multivariate time series using VAR

# The VAR method name comes from vector autoregression. So far all the other models I have used have # been univariate and forecasting the nr of confirmed COVID-19 cases based on the based observations # of the its own time series data. Next I used the VAR method to create a multivariate time series 
# model that uses all but people vaccinated: 
# - nr of tested,
# - nr of deaths,
# - nr of symptoms reported
# - nr of urgently referred to treatment.

# Using VARselect to determine the information criteria for different VAR(p)
select <- VARselect(train_set_tsdata[, 3:7], lag.max = 11, type="both")
select[["selection"]][["AIC(n)"]]
```
and SC (or BIC):
```{r var1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
select[["selection"]][["SC(n)"]]
# -> Both SC and AIC recommend using the lag length 6
```

The VAR() model lag was determined using the VARselect() and serial.test() was used to test that the residuals are uncorrelated according to a Portmanteau test. The Portmanteau test p-value is:
```{r var12, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Multivariate analysis using VAR()
fit_mv3 <- vars::VAR(train_set_tsdata[, 3:7], p=3, type="both")

# Testing the model for residual autocorrelation in the errors using a portmanteau test 
# for a model with no autocorrelation.
test <- serial.test(fit_mv3, lags.pt=10, type="PT.asymptotic")
test[["serial"]][["p.value"]]
# The null hypothesis of no autocorrelation is accepted, since the 
# p-value is higher than the significance level alpha of 0.05. 

```

VAR(3) was chosen to fit as it passes the Portmanteau test for serial autocorrelation, since the p-value is greater than the significance level of 0.05.

```{r var2, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Forecasts for 11 weeks
forecast_mv <- forecast(fit_mv3, h=11)


```

Below plot shows the visualization of the point forecast results compared to the observations in the test set:

```{r var3, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# plot results and the test data
mv_forecast_plot <- autoplot(forecast_mv[["forecast"]][["nr_confirmed"]],  
                             ylab="Test set")+ 
  autolayer(test_set_tsdata[,"nr_confirmed"])
mv_forecast_plot
```

Conclusion: the plot shows that the observed test data does not fit in the 95% prediction interval.

The difference between an observed value and its forecast was checked using accuracy(), where the forecast errors were calculated against the test set. Note that the tsCV() used on the univariate methods cannot be used for VAR() as it only works on univariate time series. 

```{r var4, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# accuracy of the forecast
accuracy_forecast_mv <- accuracy(forecast_mv, test_set_tsdata[, 3:7], d=0 , D=1)


#Adding the results as a new row to the RMSE tibble
accuracy_results <- bind_rows(accuracy_results,
                              tibble(method = "VAR", 
                                     forecasting = "Test set",  
                                     RMSE = accuracy_forecast_mv[8,"RMSE"], 
                                     MAE = accuracy_forecast_mv[8,"MAE"],
                                     MAPE = accuracy_forecast_mv[8,"MAPE"]))

# Display accuracy results: 
accuracy_results %>% 
  arrange(RMSE)
```

From the results of the accuracy metrics we can see that the model performs the worst of all the models.

### 2.4.7 Forecasts with combined model

Similarly to what we learned in the Machine Learning course in relation to predictions, the combinations of point forecast models produce typically the best results. (source: https://otexts.com/fpp2/combinations.html) Therefore, the last model I developed was a combination model. 

I calculated the combination model simply by taking the mean point forecasts of each model and then took the average of those means. The plot below visualizes the results of the different models.

I did try to run the checkresiduals() on the combination model, but it failed to find appropriate degrees of freedom for the model. I assume this due to the fact that the combination model even though it is a time series object, it is not a real forecast object.  
```{r combination, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Check residuals
#checkresiduals(Combination)
```


Plotting the train set, test set and the combination model point forecasts alone looks like this.

```{r combination1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}

######
## Forecast with combination
######
# Similarly to how in predictions, the combinations of models produced best 
# results, I'm trying the same for the forecast as the theory says it works here 
# too. (source: https://otexts.com/fpp2/combinations.html) 

# Calculating the mean of all of the models
mean_naive <- forecast_naive_case[["mean"]]
mean_ses <- forecast_ses_conf_case[["mean"]]
mean_arima <- forecast_arima_conf_case[["mean"]]
mean_var <- forecast_mv[["forecast"]][["nr_confirmed"]][["mean"]]

# Creating the combined model from the means
Combination <- (mean_naive + mean_ses + mean_arima + mean_var)/4


# plot results and the test data
plot_combined <- autoplot(Combination, series="Combination mean point forecast", ylab="Test set") + 
  autolayer(train_set_tsdata[,"nr_confirmed"]) + 
  autolayer(test_set_tsdata[,"nr_confirmed"])  
plot_combined
```

The difference between an observed value and its forecast was checked using accuracy(), where the forecast errors were calculated against the test set. But like with the VAR() I was unable to produce the time series cross-validation RMSE. Note that the tsCV() used on the univariate methods cannot be used for the combined model as it only works on functions returning an object of the class forecast. 
```{r combination2, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# accuracy of the forecast
accuracy_combined_conf_case <- accuracy(Combination, 
                                     test_set_tsdata[,"nr_confirmed"])


#Adding the results as a new row to the RMSE tibble
accuracy_results <- bind_rows(accuracy_results,
                              tibble(method = "Combination", 
                                     forecasting = "Test set",  
                                     RMSE = accuracy_combined_conf_case[1,"RMSE"], 
                                     MAE = accuracy_combined_conf_case[1,"MAE"],
                                     MAPE = accuracy_combined_conf_case[1,"MAPE"]))


# Display accuracy results: 
accuracy_results %>% 
  arrange(RMSE)

```

As would be expected the accuracy of the Combination model falls between the first two models predicting straight line and the Auto ARIMA and VAR -models.

*** 
\newpage
# 3. The Results

The plot below you can see that Naive and Simple exponential smoothing are practically the same straight line, however, all of the other models point forecast are showing increasing trend.

```{r dev_results, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
#########################################################################
## Results
#######################################################################

# Visualizing the individual models and the combined model
autoplot(train_set_tsdata[,"nr_confirmed"]) +
  autolayer(forecast_naive_case, series="Na√Øve", PI=FALSE) +
  autolayer(forecast_ses_conf_case, series="SES", PI=FALSE) +
  autolayer(forecast_arima_conf_case, series="ARIMA", PI=FALSE) +
  autolayer(forecast_mv[["forecast"]][["nr_confirmed"]], series="VAR", PI=FALSE)+
  autolayer(Combination, series="Combination") +
  autolayer(test_set_tsdata[,"nr_confirmed"], series="Test data") +
  xlab("Time") + ylab("Nr of cases confirmed") +
  ggtitle("Comparison of the forecasts across models")
```

```{r dev_results5, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE}
## Comparing all models with error metrics
accuracy_results %>% 
  group_by(forecasting) %>%
  arrange(RMSE)

## Conclusion: The multivariate VAR performed worst on all metrics and Simple 
## Exponential Smoothing performed best on all other than the RMSE of time 
## series cross-validation, but the difference to the Naive model is tiny. 
##  
## None of the forecasting models did particularly well with the best two 
## models having Mean absolute percentage error (MAPE) at best just under 42%
## and the Mean Absolute Error (MAE) being off by about 820 cases confirmed.


```

Conclusion: The multivariate VAR performed worst on all metrics and Simple 
Exponential Smoothing (SES) performed best on all other than the RMSE of time series cross-validation, but the difference to the Naive model was tiny. Theory says that the time series cross-validation RMSE comparison gives the best results. None of the forecasting models did particularly well with the best two models (naive and simple exponential smoothing) having Mean absolute percentage error (MAPE) at best 
```{r results_mape_naive, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
round(accuracy_naive_case[2,"MAPE"], digits = 0)
```
and the Mean Absolute Error (MAE) being off by about 
```{r results_mae_naive, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
round(accuracy_naive_case[2,"MAE"], digits = 0)

``` 
cases confirmed.

However, as I already know that the future observations are not a flat line, which is what both Naive and SES forecasts give, I'm going to do the validation with both Naive and Combination models.

## 3.1 Validating the Naive model with the validation data set

As the Covid-19 data is time series data and there is a limited amount of it available, test and validation sets were created by taking specified weeks out of the original data set and used as test and validation data.

Train and test set used the data up to week 6 /2021 (90%) and the last 10% of the available data (weeks 7-12/2021) were be used as validation set.

```{r validation_of_model, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
#########################################################################
## Validating the model results on the validation data set
#########################################################################


## As the Covid-19 data is time series data and there is a limited amount of 
## it available, test and validation sets were created by taking specified weeks 
## out of the training data set and used as test and validation data.

# Train and test set will use the data up to week 6 (90%) 
# the last 10% of the available data (weeks 7-12/2021) will be used as 
# validation set

# The full data set up to week 12/2021
covid_set <- combined_df %>%
  filter(year_week <= "2021-12") 


# Transforming the data set to time series format
covid_set_data <-covid_set %>%
  dplyr::select(wdate, 
                year_week, 
                nr_sympt_reported, 
                nr_reftotreat, 
                tested, 
                nr_confirmed, 
                nr_deaths, 
                vacc_peop,
                tested_per_100k,
                confirmed_per_100k,
                deaths_per_100k) %>%
  mutate(avg_daily_case_rate = confirmed_per_100k/7)



# In reality 2020 was a leap year, but with only total of 57 weeks of data, 
# it doesn't make much difference
season_duration <- 52 

## Convert the data from data frame to time series format
covid_set_tsdata <- tsibble::as_tsibble(
  covid_set_data, 
  key = year_week,
  index = wdate, 
  regular = T) 

# Creating the time series for train
covid_set_tsdata <- ts(covid_set_tsdata, start = c(2020, 8), 
                              frequency = season_duration) 

# For the simple forecasts we need separate train and test sets
# Development data set is the first 52 weeks (90% of the data set)
dev_tsdata <- head(covid_set_tsdata, 52)

# Validation data set is the last 5 weeks (10% of the data set)
validation_tsdata <- tail(covid_set_tsdata, 6)
```

For the validation of the forecasting model of the confirmed COVID-19 cases I used the the Naive model as it performed best on the test data.

The plot below shows the point forecast with the Naive model.

```{r validation_of_model1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
### For the validation of the forecasting model of the confirmed COVID-19 cases 
### I'll be using the the Naive as it performed 
### best on the test data

### Naive forecast for 6 weeks (h - periods)
forecast_naive_val <- forecast(naive(dev_tsdata[,"nr_confirmed"], h = 6))


# plot results and the validation data
autoplot(forecast_naive_val) + autolayer(validation_tsdata[,"nr_confirmed"])
```

The actual observed number of confirmed Covid-19 cases is much higher than the point forecast by the Naive model.

The performance metrics do not look too good either, even though the MAPE is lower than it was with the test data. 

```{r validation_of_model2, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Fit the model function to be used with time series cross-validation to 
# determine model with lowest cross-validation RMSE
forecast_naive_v <- function(x, h){forecast(naive(x, h = h))}

# Using tsCV() function for time series cross-validation
errors_naive_v <- tsCV(dev_tsdata[,"nr_confirmed"], 
                     forecast_naive_v, h=6)

# Calculate the RMSE of the tsCV results
RMSE_naive_v <- sqrt(mean(errors_naive_v^2, na.rm=TRUE))

# accuracy of the forecast
accuracy_forecast_cc_validation <- accuracy(forecast_naive_val, 
                                            validation_tsdata[,"nr_confirmed"])


#Adding the results as a new row to the RMSE tibble
accuracy_results <- bind_rows(accuracy_results,
                              tibble(method = "Naive", 
                                     forecasting = "Validation set",  
                                     RMSE = accuracy_forecast_cc_validation[2,"RMSE"], 
                                     MAE = accuracy_forecast_cc_validation[2,"MAE"],
                                     MAPE = accuracy_forecast_cc_validation[2,"MAPE"],
                                     RMSE_CV = RMSE_naive_v))

# Display accuracy results: 
accuracy_results %>% 
  arrange(RMSE)
```
The six weeks point forecast mean absolute error was
```{r results_mae, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
round(accuracy_forecast_cc_validation[2,"MAE"], digits = 0)
```
i.e. the forecast was that much lower than the observed values in the validation data set. In other words, the forecast did not perform well.

## 3.2 Validating the Combined model with the validation data set 

Just for reference I looked at a plot with the developed models combined with the actual validation data. 

```{r validation_of_model3, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Visualizing the individual models from the development phase with the actual 
# data 
autoplot(train_set_tsdata[,"nr_confirmed"]) +
  autolayer(forecast_naive_case, series="Naive", PI=FALSE) +
  autolayer(forecast_ses_conf_case, series="SES", PI=FALSE) +
  autolayer(forecast_arima_conf_case, series="ARIMA", PI=FALSE) +
  autolayer(forecast_mv[["forecast"]][["nr_confirmed"]], series="VAR", PI=FALSE)+
  autolayer(Combination, series="Combination") +
  autolayer(covid_set_tsdata[,"nr_confirmed"],  series="Actual data") +
  xlab("Time") + ylab("Nr of cases confirmed") +
  ggtitle("Comparison of the model forecasts with training set data")
```

On visual inspection, the original point forecast that came closest to the actual observed values was the combination model. Just to see if the combination model would have performed better than the naive model, I recalculated all of the  models on the full development data set with same parameters to get the updated combination model. 

Below plot displays a visual comparison of the developed models on the full development data set. 

```{r validation_of_model4, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Just to see if any of the models would have performed better, I'm running a 
# visual comparison of the developed models using the full development data set 
# to see, if any of them were doing better than the Na√Øve model. 

# Recalculating the all of the  models on the whole development data with same 
# parameters
forecast_ses_val <- forecast(ses(dev_tsdata[,"nr_confirmed"], h = 6))
forecast_arima_val <- forecast(auto.arima(dev_tsdata[,"nr_confirmed"]), h=6)
forecast_mv_val <- forecast(VAR(dev_tsdata[, 3:7], p=3, type="both"), h=6)

# Calculating the mean of all of the models
mean_naive_val <- forecast_naive_val[["mean"]]
mean_ses_val <- forecast_ses_val[["mean"]]
mean_arima_val <- forecast_arima_val[["mean"]]
mean_var_val <- forecast_mv_val[["forecast"]][["nr_confirmed"]][["mean"]]


# Creating the combined model from the means
Combination_val <- (mean_naive_val + mean_ses_val + mean_arima_val + 
                      mean_var_val)/4

# Visualizing the individual models and the actual data
autoplot(dev_tsdata[,"nr_confirmed"]) +
  autolayer(forecast_naive_val, series="Na√Øve", PI=FALSE) +
  autolayer(forecast_ses_val, series="SES", PI=FALSE) +
  autolayer(forecast_arima_val, series="ARIMA", PI=FALSE) +
  autolayer(forecast_mv_val[["forecast"]][["nr_confirmed"]], series="VAR", PI=FALSE)+
  autolayer(Combination_val, series="Combination val") +
  autolayer(covid_set_tsdata[,"nr_confirmed"],  series="Actual data") +
  xlab("Time") + ylab("Validation set") +
  ggtitle("Comparison of the forecasts with actual data across models")
# Conclusion: None of the models improved much on the mean point forecast with 
# the addition of the test data 

```

From the plot you could conclude that none of my models improved much on the mean point forecast with the addition of the test data.

*** 
\newpage
# 4. The Conclusions

I started to analyze the Covid-19 data and to develop the forecast model to better understand what the data was telling in comparison to what was discussed in the Finnish press regarding the current Covid-19 epidemic status and future forecast models. I used the following methods in my model development:  

* A baseline using the Na√Øve method,
* Simple exponential smoothing method,
* Auto ARIMA method,
* Multivariate vector autoregression (VAR) method,
* Combined model using mean point forecast of the other models.  

The multivariate VAR performed worst on all metrics and Simple Exponential Smoothing (SES) performed best on all other than the RMSE of time series cross-validation, but the difference to the Naive model was tiny. As theory says that the time series cross-validation RMSE comparison gives the best results, I selected the model based on naive() method for the validation.

The Naive method based model on the validation data resulted in a six weeks point forecast mean absolute error of
```{r conclusions_mae, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
round(accuracy_forecast_cc_validation[2,"MAE"], digits = 0)
```
i.e. the forecast was that much lower than the observed values in the validation data set. The time series cross-validation RMSE was:
```{r conclusions_rmse_tscv, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
round(RMSE_naive_v, digits = 0)
```
In other words, the forecast model did not perform well. I struggled to determine how important the residuals analysis was compared with the forecast accuracy. My main concern was that as the residuals for both Naive and Simple Exponential Smoothing were not indicating a good forecasting method, should I have discarded those models and only used the models with residual p-values indicating white noise? This would have meant that I should have used the ARIMA for as the final model even though it had much worse time series cross-validation RMSE than the first two models. This is something that I will need to study up in the future.

Another aspect that I did not manage to work out was the time series decomposition, i.e. if and how to deal with the trend, seasonality and cycles of the time series when developing the models using the different methods. Or if that was even possible with just 59 weeks of data. 

Even though the models developed did not perform well, analyzing the available data gave me a better understanding of the changes in the trends. It also gave me a better ability to compare / contrast our situation with e.g. the situation in San Francisco, where my friend lives. The way the Covid-19 numbers are reported differs a little country by country. Here the numbers typically reported are absolute numbers, which are difficult to compare. I found the case rate seven-day average to be the easiest to comprehend and to contrast, but to get that I had to calculate it myself. 

Trying to build a model to forecast the weekly confirmed cases of Covid-19 proved very difficult, which was to be expected. But after this attempt at building the forecast model I have an immense respect for the data scientists, who are building the models for the government and the health authorities. Their work is really important as it is being used as the bases for defining the actions to control the epidemic while trying to keep the economy as open as possible.  

As a final action I took a look at the seasonal plot now with the full set of the data.  

```{r conclusions, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Seasonal plot: confirmed cases of COVID-19 time series all data upto week 11/2021
ggseasonplot(covid_set_tsdata[,"nr_confirmed"], year.labels=TRUE, year.labels.left=TRUE) +
  ylab("Nr of confirmed cases") +
  ggtitle("Seasonal plot: Confirmed cases of COVID-19")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plot shows that the dip in the weekly number of confirmed cases between weeks 5-7 was a temporary decrease and the trend kept on going up with about the same slope after week 7 as it had before week 5. So, you could say that the breaking point of the data into training, test and validations sets was unlucky. But that just goes to prove that forecasting the future is hard, in particular, when:  

* You have no data (or don't know how to use the data) on the underlying trends or seasonality in the data.  
* You do not have the data on the other variables that influence the forecasted variable. 

Taking a last look at the metric of seven-day average for cases per 100 000 inhabitants.

```{r conclusions1, echo=FALSE, eval=TRUE, tidy = TRUE, size="small", results="asis", message=FALSE, warning=FALSE, crop = TRUE}
# Adjusted case rate seven-day average for cases per 100 000 per 100 000 inhabitants
autoplot(covid_set_tsdata[,12]) +
  geom_hline(yintercept = 1, colour="yellow") +
  geom_hline(yintercept = 4, colour="orange") +
  geom_hline(yintercept = 7, colour="red") +
  ylab("Adjusted daily rate") + xlab("Week")+
  ggtitle("The confirmed Covid-19 case rate per 100k", 
          subtitle ="Adjusted case rate seven-day average")


```

We are still after the week 12 / 2021 at the highest level on the scale i.e. the epidemic is widespread with on average more than 10 confirmed cases per day per 100 000 inhabitants. 

This project has been quite the learning experience as I accidentally selected a wrong type of data set. As I work in the field of management consulting and deal with scenario planning and forecasting in my work, I believe it was still time well spend even if it took me much longer to complete the CYO project than it should have. It also made me realize that I need to find a course that would properly teach me forecasting. I am fully aware that my models were not tuned and the methods selected were likely not using the methods best fit for the problem at hand.
